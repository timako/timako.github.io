<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="Time heals">
<meta property="og:type" content="website">
<meta property="og:title" content="Timako world">
<meta property="og:url" content="http://example.com/index.html">
<meta property="og:site_name" content="Timako world">
<meta property="og:description" content="Time heals">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="Timako">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://example.com/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'zh-CN'
  };
</script>

  <title>Timako world</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --></head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Timako world</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/07/15/A-survey-on-hair-modeling-Styling-simulation-and-rendering/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Timako">
      <meta itemprop="description" content="Time heals">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Timako world">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2023/07/15/A-survey-on-hair-modeling-Styling-simulation-and-rendering/" class="post-title-link" itemprop="url">A survey on hair modeling: Styling, simulation, and rendering</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2023-07-15 12:30:58" itemprop="dateCreated datePublished" datetime="2023-07-15T12:30:58+08:00">2023-07-15</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2023-07-19 17:40:31" itemprop="dateModified" datetime="2023-07-19T17:40:31+08:00">2023-07-19</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>这篇文章是关于头发建模的survey，在2007年发布的。主要阅读关于建模相关的部分（第二章）</p>
<h3 id="introduction">Introduction</h3>
<p>真实地表现头发的结构、运动和视觉外观仍然是一个open challenge。</p>
<p><strong>Background</strong>: 人的头部通常由大量头发组成，其中有超过
100,000
根发丝。然而，每根发丝的直径都非常小。考虑到这种二元性，研究人员研究了头发是否应该被视为整体或单独的相互作用的发丝。目前，还没有一种方法被接受为头发造型的行业标准。</p>
<p><strong>Challenge</strong>: 1.
在现实世界中，每个人的头发结构和视觉外观差异很大，这使得任何一种建模方案准确捕捉所有多样性都是一项艰巨的任务。
2.
此外，由于头发的高度复杂性，提供最佳视觉保真度的算法往往在计算上过于繁琐，无法用于具有严格性能要求的交互式应用程序。包含头发建模的各种应用都有各自的挑战和要求，例如外观、准确性或性能。
3.
此外，真实头发仍然存在未知的特性，使得目前难以创建物理上正确的建模方案。</p>
<p><strong>本次调查的工作</strong>: 1.
讨论头发建模所面临的主要挑战，并回顾过去处理这些复杂问题的方法的优点和局限性。
2.
此外，还将提供有关选择具有严格性能要求的适当头发造型方案应用程序的见解。包含头发建模的各种应用都有各自的挑战和要求，例如外观、准确性或性能。</p>
<h3 id="overview">Overview</h3>
<p>头发建模可以分为三大类：<strong>发型设计</strong>、<strong>头发模拟</strong>和<strong>头发渲染</strong>。
-
发型设计被视为对头发形状的建模，结合了头发的几何形状并指定了发丝的密度、分布和方向。
-
头发模拟涉及头发的动态运动，包括头发与物体（例如头部或身体）之间的碰撞检测，以及头发相互之间的相互作用。
-
头发渲染涉及与屏幕上头发的视觉描绘相关的颜色、阴影、光散射效果、透明度和抗锯齿问题。</p>
<p>考虑以下一般性问题来分析这些方法的几个类别：</p>
<ol type="1">
<li>头发形状：该方法可以处理长发、卷发或波浪发吗？还是仅限于更简单的短直发？</li>
<li>头发运动：该方法是否足够稳健，能够处理大的、不稳定的头发运动，这些运动可能导致头发簇的动态分组和分裂以及复杂的头发碰撞？</li>
<li>性能与视觉保真度：该方法的主要焦点是对视觉上真实的头发进行建模，快速有效地对头发进行建模，还是在虚拟头发的性能速度和视觉保真度之间提供平衡？</li>
<li>硬件要求：该方法是否依赖于特定的 GPU
功能或其他硬件限制，或者是否具有跨平台兼容性？</li>
<li>用户控制：用户对头发的控制程度如何？控制是直观的还是繁琐的？</li>
<li>头发属性：该方法能否处理各种头发属性（例如，粗与细、湿与干、硬与松）并允许这些值在整个应用过程中动态变化？</li>
</ol>
<p><strong>头发建模的应用场景</strong>：</p>
<ul>
<li><p>物理上精确的方式模拟头发的结构、运动、碰撞和其他复杂性。</p></li>
<li><p>在娱乐行业中，例如动画长片，建模物理上不可能的发型或动作通常是一个目标。在这些情况下，需要高度的用户控制来以期望的方式引导头发，由于头发体积的大小，这是一项耗时且昂贵的工作。加速和简化这一过程的方法将是头发建模研究的重要补充。</p></li>
<li><p>交互系统，例如虚拟环境和视频游戏。在这些应用中，虚拟头发的性能速度是其外观的主要重点。尽管最近的努力提高了头发建模算法的效率，但仍然希望提高所得头发的质量以捕获更多的头发形状、运动和属性。</p></li>
</ul>
<h3 id="头发发型styling">头发发型（styling）</h3>
<p>创造理想的发型通常是一个漫长、乏味且不直观的过程。在本节中，将解释控制其最终形状的真实头发的主要属性，然后介绍对虚拟头发进行造型的方法。发型设计技术可分为三个一般步骤：将头发附着在头皮上、赋予头发整体或整体形状以及管理更细的头发特性。</p>
<p><strong>头发结构和几何特性</strong>：</p>
<p>头发形状多种多样，包括天然的和人造的。根据种族群体的不同，人们可以拥有自然光滑、锯齿状、波浪形或卷曲的头发。这些几何特征可以由每根发丝的各种结构和物理参数产生，包括其横截面形状、卷曲程度或从头皮中出来的方式[4]，[5]。头发科学家将头发类型分为三大类：<strong>亚洲头发、非洲头发和白种人头发</strong>。亚洲发丝非常光滑且规则，具有圆形横截面，而非洲发丝看起来不规则，具有非常椭圆形的横截面。白种人的头发介于这两个极值之间，从光滑的头发到高度卷曲的头发。此外，大多数人通常通过刘海、马尾辫、辫子等各种方式剪发和定型头发。化妆品还可以临时（使用发胶、摩丝等）或永久（通过烫发）改变头发的形状。
、直发等），创造出各种各样的人造发型。</p>
<p>如今使用的大多数虚拟造型方法实际上并没有在其算法中考虑真实头发的物理结构。大多数虚拟造型方法并不是尝试匹配现实世界头发形状的生成过程，而是尝试将最终结果与现实世界头发的外观相匹配。因此，虚拟造型技术<strong>不适合</strong>需要<strong>物理上正确</strong>的头发结构模型的应用，而是适合需要<strong>视觉上合理</strong>的解决方案的应用。然而，最近人们一直在努力创建造型方法，通过考虑已知的真实物理头发属性
[6] 并模仿更自然的用户与头发的交互
[7]，更准确地反映发型生成的现实过程。尽管前景光明，但这些努力仍处于早期阶段。</p>
<h3 id="将头发附在头皮上">将头发附在头皮上</h3>
<p>由于构成人类头发的单个发丝数量较多，因此手动将每根发丝放置在头皮上是极其繁琐的。为了简化该过程，人们开发了许多直观的技术，将头发以
2D 或 3D 方式放置在头皮上。</p>
<p><strong>2D Placement</strong></p>
<p>在一些造型方法中，发束并不直接放置在头部模型的表面上。相反，用户以交互方式在
2D map上绘制头发位置，随后使用映射函数将其投影到 3D
模型上。将发丝基（strand base）映射到头皮 3D 轮廓的球形映射是流行的方法
[2]、[8]。</p>
<p><img src="/pic/hairmodel/1.png" /></p>
<p>或者，Kim 和 Neumann [9] 定义了一个 2D
参数化补丁，用户将其包裹在头部模型上，如图 1
所示。用户可以交互地指定样条补丁（spline
patch）的每个控制点。在由补丁的两个参数坐标定义的 2D
空间中，用户可以放置各种头发簇。</p>
<p><strong>[?]</strong> 将发根放置在 2D
几何体上对于用户来说很容易并且具有灵活性，但是将 2D 发根映射到 3D
弯曲头皮上可能会导致扭曲。班多等人。
[10]使用谐波映射，并通过使用世界空间中头皮上对应点之间的距离而不是它们的
2D 地图位置来基于泊松盘分布来分布根粒子来补偿映射失真。</p>
<p><strong>3D Placement</strong></p>
<p>另一种方法是将发根直接 3D 放置到头皮上。 Patrick 和 Bangay [11]
提出了一个交互式界面，用户可以在其中选择头部模型的三角形。这组选定的三角形定义了头皮，即头发将附着的头部网格区域，头皮的每个三角形都是一缕头发的初始部分。</p>
<p><strong>股线在头皮上的分布</strong></p>
<p>放置发丝的一种流行方法是在头皮上均匀分布，因为它可以很好地近似真实的头发分布。一些基于缕的方法将发根随机分布在根缕部分覆盖的头皮的每个区域内[12]、[13]、[14]，但如果缕部分重叠，则在重叠区域中会产生更高的头发密度，这可能会产生分散注意力的结果。为了保证头发在整个头皮上的均匀分布，Kim
和 Neumann [9]
将头发均匀地分布在头皮上，然后将每个生成的发根分配给其所有者簇。一些方法还允许用户在头皮上绘制局部头发密度[15]，[13]。通过将密度值表示为颜色级别，可以以
3D
方式可视化头发密度。控制这个参数有助于产生进一步的发型，例如稀疏的头发。
Hernandez 和 Rudomin [15]
扩展了绘画界面以控制进一步的头发特征，例如长度或卷曲度。</p>
<h3 id="全局头发形状生成">全局头发形状生成</h3>
<p>一旦头发被放置在头皮上，就必须给它一个所需的整体形状，这通常是通过基于几何、基于物理或基于图像的技术来完成的，本节将对此进行解释和评估。</p>
<p><strong>基于几何的发型</strong></p>
<p><img src="/pic/hairmodel/2.png" /></p>
<p>基于几何的发型设计方法主要依赖于头发的参数化表示，以便允许用户通过直观且易于使用的界面交互式地定位头发组。这些参数化表示可以涉及以三棱柱或广义圆柱体的形式表示头发或缕的表面。</p>
<p><strong>a) 参数化曲面</strong>。使用 2D
曲面来表示发束组已成为头发建模的常用方法
[16]、[17]、[18]。通常，这些方法使用参数化曲面（例如 NURBS
曲面）的补丁来减少用于对一段头发建模的几何对象的数量。这种方法还有助于加速头发模拟和渲染。这些
NURBS
曲面（通常称为发带）被赋予头皮上的位置、方向和结的权重，以定义所需的头发形状。然后使用纹理映射和
Alpha
映射使条带看起来更像发丝。可以通过指定一些控制曲线或发束来创建完整的发型。然后将这些发束的控制点水平和垂直连接以形成一条带。虽然这种方法可以用于快速发型生成和模拟，但由于条带的平面表示，可以建模的发型类型受到限制（见图2a）。</p>
<p>为了缓解头发的扁平外观，Liang 和 Huang [17] 使用三个多边形网格将 2D
条带扭曲成 U 形，从而为头发提供更多体积。在此方法中，将 2D
条带的每个顶点投影到头皮上，然后将顶点连接到其投影。额外的几何细节也可以从表面表示中提取。
Kim 和 Neumann [19] 开发了一种称为薄壳体积 (Thin Shell Volume, TSV)
的模型，该模型从参数化表面开始创建发型。通过沿法线方向偏移表面来增加头发的厚度。然后将各个发束分布在
TSV 内（见图 2b）。使用 Noble 和 Tang [18] 的方法可以在 NURBS
曲面上生成额外的头发簇。从已成形为所需发型的 NURBS 体积开始，然后沿着
NURBS
体积的等值曲线生成关键头发曲线。从关键头发曲线挤出的轮廓曲线会创建额外的簇，然后可以独立于原始
NURBS
曲面对其进行动画处理。这种方法为使用表面方法捕获的头发形状和运动类型增加了更大的灵活性。</p>
<p><strong>b) 缕和广义圆柱（Wisps and Generalized
Cylinders.）</strong>。束和广义圆柱体已被用作直观的方法来控制多束发束的定位和形状[14]，[20]，[21]，[22]，[13]。这些方法减少了定义发型所需的控制参数的数量。一组发束往往依赖于一条一般空间曲线的定位，该曲线用作限定广义圆柱体（也称为毛发簇）的横截面的半径函数的中心。簇头发模型是由分布在这些广义圆柱体内部的发束创建的（见图
3）。然后，用户可以通过编辑一般曲线的位置来控制发束的形状。这些簇或缕可以创造出许多流行的发型，从许多非洲发型的辫子和扭曲[22]到马尾辫等受约束的形状。一些不依赖于分组为固定簇组的股线的更复杂的发型更难以用这些方法来实现。此外，虽然它们为用户提供了直观的控制，但发型的塑造通常很乏味，因为创建发型的时间通常与最终风格的复杂性相关。</p>
<p><strong>c)
多分辨率编辑</strong>。复杂的头发几何形状也可以用广义圆柱体的层次结构来表示[9]、[23]，允许用户在形状建模中选择所需的控制级别。较高级别的集群为快速全局形状编辑提供了有效的方法，而较低级别的集群操作则允许直接控制详细的头发几何形状
- 直至每根发丝。 Kim 和 Neumann [9]
进一步表明，他们的多分辨率方法可以生成复杂的发型，例如使用复制粘贴工具将一个簇的详细局部几何形状转移到其他簇的卷曲簇（见图
4）。</p>
<p><strong>基于物理的发型</strong></p>
<p>一些发型设计技术与基于物理的头发动画密切相关。这些方法依赖于方法中几个关键参数的规范，从控制单根头发的悬臂梁到控制整个头发体积的流体流动方法。这些方法通常减少用户对最终发型的直接控制量。</p>
<ol type="a">
<li><p>悬臂梁。在材料强度领域，悬臂梁被定义为仅一端嵌入固定支撑件中的直梁。安乔等人。
[3]认为这与人类发丝的情况类似，发丝固定在毛孔处，另一端是自由的。考虑到重力是弯曲的主要来源，该方法模拟悬臂梁的简化静力学以获得一根发丝静止时的姿势。然而，由于使用线性模型，需要对线施加额外的力以获得正确的最终形状。</p></li>
<li><p>流体流动。 Hadap 和 Magnat-Thalmann [24]
基于静态头发形状类似于障碍物周围流体流动快照的想法，将静态发型建模为流体流动的流线。用户通过在头发体积周围放置流、涡流和源来创建发型。例如，使用涡流在所需位置的头发中形成卷曲（见图
5）。</p></li>
<li><p>设置矢量和运动场的样式。
Yu[8]观察到矢量场和头发在特定点上都具有明确的方向，同时它们也是体积数据；这促使他使用静态
3D 矢量场来建模发型，见图
6a。给定通过叠加程序定义的向量场基元生成的全局场，通过追踪向量场的场线来提取发丝。一根发丝从头皮上的指定位置开始，然后沿着矢量场累积矢量的方向以一定的步长生长，直到达到所需的长度。类似地，粒子可以在运动场中用于塑造股线[25]。粒子被赋予固定的寿命并通过运动场进行追踪。粒子的历史包括整根发丝；改变粒子的寿命就会改变头发的长度。</p></li>
</ol>
<p>Choe 和 Ko [13]
还使用向量场来计算全局头发位置，同时考虑头发弹性。他们的算法计算头发关节角度，该角度最能解释矢量场的影响和发束的自然趋势，以检索其静止位置。该方法的另一个重要特征是用户能够定义头发约束。头发约束导致在
3D
空间的一部分上生成约束向量场，该约束向量场随后按权重参数的比例修改原始向量场。头发变形是通过使用先前应用于修改后的矢量场的算法来计算的。实际上，用户可以指定三种类型的约束：点约束、轨迹约束和方向约束。事实证明，头发约束对于创建涉及马尾辫、发束或辫子的复杂发型非常有用，如图
1 和 1 所示。 6b和6c。</p>
<p><strong>从图像生成发型</strong></p>
<p>最近的发型生成方法提出了一种基于图像自动重建头发来生成发型的替代方法。
a)
从照片生成头发。孔等人。是第一个使用真发图片自动创建发型的人[26]。他们的方法仅仅是几何方法，包括从对象头发的不同角度构建
3D
头发体积。然后使用启发式在该体积内生成发丝，该启发式不能确保头发方向性的忠实性。这种方法最适合简单的发型。</p>
<p>格拉布利等人。引入了一种利用头发照明来从图像中捕获头发局部方向的方法[27]。他们的系统通过研究受试者头发在各种受控照明条件下的反射率来工作。固定视点使他们能够处理完美配准的图像。通过考虑单个视点并使用单个滤波器来确定发丝的方向，该方法仅部分重建头发。巴黎等人。通过考虑各种视点以及多个定向过滤器，将这种方法[28]扩展为更准确的方法；他们的策略主要包括在给定的
2D
位置上测试多个滤波器，并选择能为该位置提供最可靠结果的滤波器。该方法捕获头发可见部分的局部方向，从而产生与原始发型视觉上忠实的结果（见图
7）。魏等人。
[29]随后通过利用多个视点固有的几何约束来提高该方法的灵活性，这证明足以检索头发模型，而不需要受控的照明条件或复杂的设置。</p>
<ol start="2" type="a">
<li>从草图生成头发。毛等人。
[30]开发了一个基于草图的系统，专门用于卡通发型建模。给定 3D
头部模型，用户以交互方式在头皮上绘制应放置头发的边界区域。然后，用户在头部的前视图周围绘制目标发型的轮廓。系统生成代表发型边界的轮廓表面。在轮廓表面和头皮之间生成代表头发簇的曲线。这些曲线成为代表大部分头发的多边形条的脊柱，类似于[16]、[17]使用的条。</li>
</ol>
<p>这个基于草图的系统只需用户最少的输入即可快速创建卡通发型。然而，用于表示头发的条带或簇多边形并不适合建模更复杂的发型，例如现实世界中可观察到的发型。</p>
<p><img src="/pic/hairmodel/3.png" /></p>
<p><strong>估计</strong></p>
<p>本节中描述的每种全局头发造型方法都适用于不同情况下的头发造型。表 1
显示了几种全局整形方法在头发形状灵活性、用户控制以及手动设置或输入时间方面的比较。算法可以建模的头发形状范围越大，其在实践中的适用性就越广泛。为了便于将精确的细节放置在头发中所需的位置，用户控制的水平很重要。此外，虽然一些造型方法可以通过自动处理快速捕捉头发形状，但其他方法则需要耗时的手动设置或由用户输入。</p>
<p>如表 1
所示，基于几何形状的发型设计技术，例如通过广义圆柱体或参数化曲面，通常可以让用户对头发有很大程度的控制；然而，由于头发体积庞大且复杂，手动定位头发可能是一项乏味且耗时的任务。用户使用
Kim 和 Neumman [9]
提出的多分辨率广义圆柱方法创建发型的时间在几分钟到几个小时之间，具体取决于头发形状的复杂程度。虽然参数化曲面通常提供快速的发型创建方法，但由于
2D
曲面表示，结果往往仅限于平坦、直的发型。或者，一缕或广义的圆柱体可以模拟许多直或卷的发型形状。</p>
<p>本节中描述的每种全局头发造型方法都适用于不同情况下的头发造型。表 1
显示了几种全局整形方法在头发形状灵活性、用户控制以及手动设置或输入时间方面的比较。算法可以建模的头发形状范围越大，其在实践中的适用性就越广泛。为了便于将精确的细节放置在头发中所需的位置，用户控制的水平很重要。此外，虽然一些造型方法可以通过自动处理快速捕捉头发形状，但其他方法则需要耗时的手动设置或由用户输入。通过基于物理的技术（例如通过流体流动或矢量场）控制头发的体积通常需要用户不太繁琐的输入；然而，许多复杂发型的细节通常很难通过这种互动来捕捉。许多参数对于发型设计来说可能不直观，并且与基于几何的方法相比，用户通常对发型创建的具体控制较少。</p>
<p>Wei
等人已证明，即使设置相对简单，从图像生成发型也是一个高度自动化的过程。
[29]。从图像创建的最终发型可能非常令人印象深刻，但这些方法的局限性在于它们来自现实世界中必须存在的发型，使得建模的样式范围通常不如基于几何或基于物理的方法灵活。从草图生成的发型可以让最终的头发形状具有更多的创造力，尽管如果没有繁琐的用户参与，就不可能实现特定的更精细的细节，例如辫子头发。</p>
<p>最近有一些技术建立在不同方法的优势之上。例如，Choe 和 Ko [13]
的工作以缕状形式对头发进行建模，其中用户编辑控制缕状形状的原型线，但也利用矢量场和头发约束来实现复杂的头发形状，例如辫子、发髻和马尾辫。虽然没有提供手动输入的确切时间，但用户输入的量仍然被认为很高，并且是整个虚拟发型设计过程中最耗时的方面。</p>
<h3 id="管理更细的头发特性">管理更细的头发特性</h3>
<p>在头发被赋予整体形状后，通常需要改变头发的一些更精细、更局部的属性，以创建更真实的外观（例如卷曲或体积）或捕获头发的其他特征，例如水或定型产品的影响。在实践中，大多数控制更精细细节的技术都与几何或基于物理的方法结合使用来定义全局头发形状（第
2.3.1 和 2.3.2 节）。</p>
<p><strong>卷发和波浪的细节</strong></p>
<p>一旦定义了全局形状，可能需要添加局部细节，例如卷曲、波浪或噪音，以实现头发的自然外观。
Yu[8]通过使用一类三角偏移函数生成不同类型的头发卷曲。因此，可以通过控制不同的几何参数（例如偏移函数的幅度、频率或相位）来创建各种发型。为了防止头发看起来太均匀，偏移参数与从一个头发簇到另一个头发簇不同的随机项相结合（见图
8a）。类似地，通过将分离行为结合到单个发束中，可以使通过流体流动成形的头发产生更自然的外观，该分离行为允许发束基于概率函数脱离流体流[24]。</p>
<p><img src="/pic/hairmodel/4.png" /></p>
<p>Choe 和 Ko [13]
用几缕头发建模发型，每缕头发的整体形状由主发束的形状决定。在一缕内，线之间的相似程度由长度分布、偏差半径函数和模糊值（
a length distribution, a deviation radius function, and a fuzziness
value.）控制。主链的几何形状被分解为轮廓组件和细节组件。细节组件是使用马尔可夫链过程从原型链构建的，其中主链和原型链之间的相似程度可以通过吉布斯分布来控制。因此，最终的发型是全局一致的，同时包含极大地有助于其真实感的细微变化，如图
8b 所示。</p>
<p>这些局部形状变化的方法有助于减轻虚拟头发的合成外观；然而，由于它们中的大多数都包含某种形式的随机生成，因此用户对更精细的细节的控制较少。这种半自动过程有助于加速发型的创建，因为如果手动执行这些微小的细节可能需要许多工时。另一方面，如果链受到扰动导致不自然的碰撞，则随机生成也可能导致不必要的伪影。此外，这些方法没有考虑计算头发几何形状的物理头发属性，尽管众所周知，第
2.1 节中描述的这些特征对头发形状有很大影响[4]、[5]。</p>
<p>为了自动生成自然头发的精细几何形状，包括波浪或卷发，Bertails 等人。
[6]最近推出了一种新的发型设计方法，使用静态弹性杆的机械精确模型（基尔霍夫模型）。该方法基于势能最小化，考虑了头发的自然卷曲以及头发纤维横截面的椭圆度（见图
9）。虽然不适合创建复杂的发型，但这种方法有望实现更准确的发型生成过程，并考虑到个体头发纤维的特性。因此，它对于化妆品原型设计很有用。最近，这种方法被扩展到头发动力学（参见第
3.2.4 节）。</p>
<p><strong>生成头发体积</strong></p>
<p>大多数基于几何的发型设计方法通过使用体积基元隐式地赋予头发体积（参见第
2.3.1
节），而基于物理的方法通常会考虑头发自碰撞，以产生体积发型。将头发视为连续介质的方法
[25]、[24]、[8]、[13]
通过使用再现发丝之间碰撞效果的连续特性（例如通过矢量场或流体动力学。当发丝变得更靠近时，这些技术要么由于矢量或运动场的布局而阻止它们相交，要么促进排斥运动以使它们彼此分开。</p>
<p>由于检测发丝之间的碰撞可能很困难，而且至少非常耗时，Lee 和 Ko [31]
开发了一种技术，可以增加发型的体积，而无需定位发丝之间的特定交叉点。这个想法是，头上纬度较高的发丝会覆盖毛孔较低的发丝。创建多个头部外壳层，其尺寸与原始头部几何形状不同。根据发丝毛孔的位置，将发丝与特定的外壳进行检查。然后使用头发-头部碰撞检测和响应算法。此方法仅适用于保持垂直方向的准静态头部的情况。</p>
<p><strong>造型产品和水效果建模</strong></p>
<p>发胶、慕斯、凝胶等定型产品对头发的外观有显著的影响，包括头发运动后的发型恢复、头发整体运动的僵硬、固定剂产品的黏附性导致的发束大团块、头发体积的变化等。</p>
<p>Lee和Ko [ 31
]开发了一种方法来模拟发胶对发型的影响。当头发由于外力或头部运动而移动时，发型力用于使发型恢复。因此，一个初始的发型可以在运动后恢复。当凝胶应用于头发时，渴望的是保留变形的发型，而不是回到最初的风格。该算法在仿真过程中通过更新造型力来保持变形后的形状。或者，可折断的静态链接或动态键可以用来捕获发型恢复，通过在头发附近的部分之间施加额外的弹簧力来模仿发型产品所产生的头发的额外聚集[
32 ]，[ 33 ]。</p>
<p>发型产品还增加了头发运动的刚度，允许卷曲的头发部分与发型产品一起应用，以保留紧密的卷发作为头发的运动。通过使用双骨架模型来模拟头发，可以使用单独的弹簧力来控制头发线的弯曲和卷发的拉伸[
33 ]。然后，造型产品可以独立地改变弹簧刚度，以产生预期的效果。</p>
<p>水也会极大地改变头发的外观、形状和运动。由于水被吸收到头发中，头发的质量增加了45
%，而其弹性模量降低了10倍，从而导致了更易变形和弹性较小的材料[ 34
]。此外，随着头发的潮湿，头发的体积减小，这是因为由于水的键合性质，头发之间的距离很近。Bertails等人[
6
]在其基于物理的静态模型中，通过简单地修改头发变湿时实际变化的相关物理参数：每根纤维的质量和杨氏模量，很容易地将水对头发的影响纳入其中。Ward等[
33
]利用其双骨架系统，通过在头发中添加水，自动调节头发沿骨架的质量，对动态湿头发进行建模。
质量的增加导致头发的整体运动受限和卷发的伸长。灵活的几何结构允许通过改变模拟(见图10)中使用的股线组的半径来动态改变头发的体积。</p>
<p>Ward等[ 7
]介绍的交互式虚拟发型系统说明了如何使用水和发型产品通过3D界面动态地改变头发的外观和行为，允许用户执行常见的发廊应用程序(如润湿、切割、吹干等)，以便直观地创建最终的发型。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/07/14/AutoHair-paper-reading/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Timako">
      <meta itemprop="description" content="Time heals">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Timako world">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2023/07/14/AutoHair-paper-reading/" class="post-title-link" itemprop="url">AutoHair paper reading</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2023-07-14 12:44:43" itemprop="dateCreated datePublished" datetime="2023-07-14T12:44:43+08:00">2023-07-14</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2023-07-19 17:40:35" itemprop="dateModified" datetime="2023-07-19T17:40:35+08:00">2023-07-19</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>这篇文章是HairGAN的前身，也是周昆老师在研究进展课上提到过的工作。</p>
<h3 id="概览">概览</h3>
<p>该文章首次提出了基于单张图片，在无需用户交互输入情况下，快速重建3D头发模型（几何形状）的方法。该方法的核心包括：</p>
<ul>
<li>基于深度学习网络的发型解析算法，支持基于单张2D肖像头发图片，输出头发的2D分割掩码图(Mask)，以及生长方向描述。为支持神经网络的训练，构建了手动标注的2D发型图片库，标注信息包括了头发分割掩码图、生长方向图、基于聚类方法得到的头发分类类型；</li>
<li>3D头发模型库。基于少量3D发型扩展生成大量不同类型的3D头发模型库。数据库包含了3D头发模型、正向(front-view)头发分割图、不同角度的渲染图，以及发型生长方向图等元数据；</li>
<li>基于数据驱动发型匹配和头发建模方法，支持2D头发图片到3D头发模型的快速搜索匹配，以及基于分割模版和头发生成信息，自动拟合变形得到与2D输入形状一致的3D头发模型。</li>
</ul>
<h3 id="之前的工作">之前的工作</h3>
<p>仅根据一张自拍照片以及一些用户指定的笔画来建模 3D
头发的技术支持许多有趣的消费级应用，包括肖像弹出窗口和发型虚拟试戴
，虚拟理发和图像空间物理头发模拟，以及肖像重新照明和 3D
打印肖像浮雕。然而，现有技术的一个主要缺点是需要用户交互，这需要与精心设计的<strong>先验</strong>结合起来，以解决单视图头发几何重建的不适定性。例如，所有现有技术都要求用户从输入图像中分割头发区域；用户需要从根部到尖端绘制几笔来显示头发的连接性和拓扑结构，整个交互过程可能长达五分钟。用户交互的需求阻碍了这些技术更广泛的消费级应用。</p>
<h3 id="工作概览">工作概览</h3>
<p>给定单个图像作为输入，的管道会自动生成头发分割以及方向图（§4）。然后将此信息与预先计算的
3D 头发模型范例 (§5) 相结合，生成最终的股线级头发模型 (§6)。</p>
<p>具体来说： 1.
首先使用全局分类器将输入图像分类为几个头发空间分布类别之一（文章中使用了四个）。接下来，作者使用一种新颖的深度神经网络生成精确的头发掩模和粗略量化的方向图（§4.3），该神经网络根据大量带注释的头发图像（§4.1）进行训练，并针对当前的头发分布类别进行定制。
2. 然后，在公共存储库 (§5) 生成的大量 3D 头发样本中执行基于图像的搜索
(§6.1)，以获得一些最佳匹配候选者。使用边界对应关系对这些候选模型进行进一步变形和细化，并选择方向图最接近输入图像的模型（§6.2）。
3. 最后，在选定的 3D 模型和输入图像的方向估计的指导下生成 3D
发丝（§6.3）。</p>
<h3 id="头发图像解析">头发图像解析</h3>
<h4 id="图像预处理">图像预处理</h4>
<p>启发式地，人们可以假设：
1）头发分布在靠近面部的区域，很容易被检测到（例如，[Wang et al. 2011;
Smith et al. 2013]）；但这对于长发无效；
2）头发有其狭窄的先验颜色分布[Wang et al.
2011];然而，这对于环境照明和染发来说并不稳健，更不用说与背景的模糊性了；
3）头发像素通常局部高频；然而，对于深色头发和低质量输入，它们也可以是超级扁平的。
因此：基于简单的启发式规则几乎不可能找到鲁棒的头发分割解决方案。</p>
<p>作者的解决方式是： 1. 预处理大量的头发图像（4.1） 2.
从图像中计算不同类别的头发的不同空间分布（4.2） 3.
训练分类器来确定头发分布类别，以及<strong>每个类别的神经网络</strong>，以生成相应的头发分割以及单个图像的方向图（4.3）。</p>
<p>作者从Flickr收集了大约100K 肖像图像，并按照以下步骤准备训练图像：</p>
<ol type="1">
<li>图像选择。选择了 20,000
张清晰可见人脸和头发的高分辨率自然照片，排除了脸部/头发过度遮挡、照明不足或不常见风格化的照片</li>
<li>掩模分割。使用基于笔划的交互式分割和 PaintSelection
的抠图工具为每个选定的图像获得二值化的头发区域掩模。</li>
<li>方向指导。对于每个选定的图像，通过沿着子区域边界绘制笔划，将头发区域
Mh
手动分割为几个具有连贯且平滑变化的头发生长方向的子区域。在每个子区域内，添加单个笔划来注释链生长方向。扩散笔划方向以填充整个子区域，并将其与密集计算的每像素无方向方向图(Densely-calculated
per-pixel nondirectional orientation map) O 相结合以获得最终的方向图
D。最后，将方向范围<span class="math inline">\([0, 2π)\)</span>离散化为
4 个 bin（即 <span class="math inline">\([0, 0.5π\)</span>、<span
class="math inline">\([0.5π, π)\)</span>、<span
class="math inline">\([π, 1.5π])\)</span>、<span class="math inline">\([
1.5π,2π)\)</span>）并对每个像素分配标签，得到方向标签图Md。不在头发区域的像素也分配特定的标签。</li>
</ol>
<p>在该过程结束时，得到 20,000
张带注释的肖像图像，每张图像都有两个标签图：一个用于头发掩模的二值掩模图
Mh，一个用于头发中量化的<strong>每像素</strong>头发生长方向的方向标签图
Md地区。本文中的方向图被定义为头发的生长方向，从头皮表面到发梢，因为现在的方法只能定义头发的“空间方向”而不能很好地得到“生长方向”（有向的空间方向）因此只要能给出粗量化的标签图，就可以得到实际的带方向的头发生长方向。</p>
<h4 id="头发分布类">头发分布类</h4>
<p>自动头发分割的部分困难来自于无数的头发形状和分布。通过利用参考人脸定位，将所有图像聚类为一些头发分布的模糊簇。每个簇代表一类在面部周围具有相似空间分布的发型，用于高质量的头发解析，如下一小节将进行描述。首先详细介绍如何计算头发分布类别，如下所示。对于每个带注释的图像
I，首先使用 [Cao 等人的稳健面部对齐方法] 检测和定位面部标志。
2012]，并在参考面坐标系中严格配准I到I′，以进行比例和向上方向校正。如图 3
所示，然后在极坐标系中围绕 ace 的原点（面心）构建具有 nH 个
bin（在的实现中为 16 个）的圆形分布直方图 H。每个 bin 都会计算极角落在该
bin
范围内的头发像素。归一化后，H可以视为图像的nH维分布特征向量。最后，根据这些分布特征向量，使用
K 均值聚类将选定的图像聚类为几组（在的实验中为四组）[Lloyd
1982]。在聚类过程中，使用基于 L1 的Earth Mover’s Distance (EMD)
计算两个直方图 Ha 和 Hb 之间的距离 [Ling 和 Okada 2007]： <span
class="math display">\[
\begin{aligned}d_{\mathcal{H}}(\mathcal{H}_a,\mathcal{H}_b)&amp;=\min_{a(i,j)}\sum_{i=1}^{n_{\mathcal{H}}}\sum_{j=1}^{n_{\mathcal{H}}}\alpha(i,j)d_b(i,j),\\\mathrm{s.t.}\quad\sum_i\alpha(i,j)&amp;=\mathcal{H}_i,\quad\sum_j\alpha(i,j)=\mathcal{H}_j,\quad\alpha(i,j)\geq0,\end{aligned}
\]</span></p>
<h4 id="计算层次化网络">计算层次化网络</h4>
<p>利用第 4.2
节中计算的预处理训练图像和头发分布类，以自上而下的层次结构设计头发图像解析管道：</p>
<ol type="1">
<li>头发区域估计。人脸检测器 [Cao 等人。
2012]用于生成头发区域候选（即头发边界框）的估计，该估计将由下一层的头发分类器使用。</li>
<li>头发分类器。对于候选头发区域，R-CNN（具有 CNN 特征的区域）网络
[Girshick 等人。 2014]
被训练来选择一个具有最大分数的头发分布类别，并将图像传递到其特定于类别的分割器。</li>
<li>头发掩码分割器。每类头发掩码分割器使用 DCNN
进行训练，输出下采样概率图。对地图进行上采样并应用完全连接的 CRF
[Krähenbühl 和 Koltun 2011] 来细化经常过度平滑的初始分割结果 M。</li>
<li>头发方向预测器。除了掩模分割器之外，方向预测器还生成方向标签图。然后，对方向标签图进行上采样，并将其与掩模内的像素方向相结合，作为最终方向图
D，如第 4.1 节中所示。</li>
</ol>
<p>管道的结构设计如下：</p>
<ol type="1">
<li>在管道的开始，运行 Cao
等人的面部对齐算法检测一组面部特征点，这些特征点用于在参考面部坐标系中配准图像，如§4.2中所示。</li>
<li>接下来，对于每个头发分布类别，通过旋转和缩放来调整图像中脸部的 20
个典型头发区域边界框，从而产生图像的一组候选头发区域。典型的头发区域边界框是通过对每个分布类内的训练图像的头发边界框进行预聚类来生成的。</li>
<li>然后，候选区域被裁剪并输入头发分类器进行独立处理。注意到，这种头发区域估计仅在测试阶段执行一次，而不是在训练阶段执行一次，在训练阶段，每个训练图像的头发区域是已知的。</li>
</ol>
<h3 id="hair-model-exemplars">Hair Model Exemplars</h3>
<p>利用3D头发数据集辅助头发建模是一种数据驱动(data-driven)的方法，由于数据集中包含了头发结构的先验结构信息，该类方法可以用于提高重建的效果。该文提出的3D头发数据集主要用于发型图片的检索，确定发型的大致形状和结构，用于下一步的头发丝细化，而非用于深度学习网络的训练。</p>
<p>在文献[4]中，头发融合是在用户使用笔刷指定头发大致方向后进行，用户需要等待时间。在本文头发的重组融合移至预计算阶段，避免了用户交互以及实时计算开销。通过穷尽式的组合方式构建大量的合理发型样本。</p>
<h3 id="data-driven-hair-modeling">Data-Driven Hair Modeling</h3>
<p>给定包含头发的肖像图像，通过图像解析计算，得到头发区域图和头发生长预测图。依据图像解析结果，在头发3D合成数据集{}中检索匹配若干候选模型。候选模型根据头发边界对应关系进行变形和改进，得到与输入图片中头发最相似的3D模型。根据头发生长信息对最佳相似模型进行细化，得到最终精细的3D发丝模型。</p>
<h4 id="image-based-candidate-search">Image-Based Candidate Search</h4>
<p>根据输入图像重建3D头发的第一步即在3D头发数据集中搜索相似的头发模型。由于数据集中头发模型数量较多，逐一比较效率太低。由于不同发型之间结构和形状差异较大，可以通过两步早期测试排除差异过大的头发模型，提高搜索效率。</p>
<h4 id="hair-shape-deformation">Hair Shape Deformation</h4>
<p>候选匹配模型需要进行变形使其能更好的与输入图片中头发匹配。具体的，首先计算头发模型的图像投影轮廓与图片中头发轮廓的对应关系，然后通过全局光滑插值计算两个发型区域的对应关系，最后基于全局对应关系，变形和优化3D头发模型，使之与输入图片中发型更为接近。</p>
<h3 id="final-hair-generation">Final Hair Generation</h3>
<p>给定变形后的候选头发模型集<span
class="math inline">\({H&#39;}\)</span>，采用前面章节介绍的缩略图匹配方法，从集合中选择与输入图发型方向最一致的模型作为最终的头发模型。</p>
<p>将最终候选模型在模型包围盒空间内转化为一个三维空间方向场，并以指定的方向以及头皮法线作为约束，将方向扩散到整个发型内部。自头皮均匀采样，并按照发型方向生成10,000条发丝。生成的发丝再依据方向场变形[4]得到最终效果。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/07/14/HairGAN-paper-reading/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Timako">
      <meta itemprop="description" content="Time heals">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Timako world">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2023/07/14/HairGAN-paper-reading/" class="post-title-link" itemprop="url">HairGANs paper reading</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2023-07-14 11:59:41" itemprop="dateCreated datePublished" datetime="2023-07-14T11:59:41+08:00">2023-07-14</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2023-07-19 17:40:41" itemprop="dateModified" datetime="2023-07-19T17:40:41+08:00">2023-07-19</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>这篇文章是我在找综述的时候看见友怡老师写的，跟老师给我的毕业论文题目很相近，优先阅读一下</p>
<h3 id="introduction">Introduction</h3>
<p>数据驱动的方法存在一些问题。 1.
首先，头发数据库对大存储的要求限制了其在移动设备等资源受限平台上的应用。
2.
其次，建模结果的质量取决于从数量有限的数据库中搜索到的头发样本。最终结果的结构仍然受到最初检索到的样本的限制。
3.
此外，贪心搜索过程缓慢且难以平衡局部细节相似性和全局形状相似性之间的选择标准。</p>
<p>生成对抗网络能够比 CNN
和自动编码器模型更好地捕获模型分布，因为它们的对抗训练性质倾向于自行学习更通用的距离度量，而不是手动编码[14]。</p>
<p>论文的工作： 1. 介绍了用于单视图头发建模的 GAN 架构。 2. GAN 将 2D
方向图转换为 3D 体积场，对发丝的占用和方向信息进行编码； 3.
建议在生成器网络的设计中加入一个维度扩展层，将一系列 2D 特征转换为单通道
3D 特征； 4. 通过考虑鉴别器的输出和潜在特征来优化生成器参数。</p>
<h3 id="概览">概览</h3>
<p>我们首先澄清我们的统一模型空间，其中所有合成头发数据库和相同的胸围模型都对齐。基于统一的模型空间，我们生成与相应的
2D 方向和置信图耦合的真实 3D 体积场的训练数据（§4）。接下来，我们介绍
Hair-GAN 的架构和损失函数（§5）。与原始的 GAN
类似，我们的网络也由判别器和生成器组成。给定真实的头发图像作为输入，通过使用我们训练有素的头发生成器，我们可以根据
2D 方向和置信图以及胸围深度图（所有这些都从图像中提取）恢复 3D
头发结构，并且最后合成一个高质量的3D头发模型（§6）。</p>
<h3 id="数据准备">数据准备</h3>
<p>受[19]的启发，我们将发型视为分布在人体周围的局部风格图案的融合，与<code>[1][2][11]</code>相反，其中不同的发型被视为不同风格的发丝组合。与之前的研究类似，我们收集了一个原始头发数据集，其中包含[1]提供的约
300 个 3D
人造头发模型，这些模型已经与相同的半身模型对齐。我们定义一个统一的模型空间（§4.1）来准备我们的训练数据（§4.2），包括真实的
3D 体积场 Y 和 2D 头发信息图 X 。</p>
<p><strong>统一模型空间</strong></p>
<p><img src="/pic/hairgans/1.png" /></p>
<p>我们将边界框定义为模型空间的边界，在其中生成真实的 3D
头发方向体积并捕获 2D 头发方向和置信度图。图 2 以我们定义的边界框和 2D
捕获为例。</p>
<p><strong>包围盒</strong>：模型空间由考虑到半身模型和除一些极长头发（手动选择）之外的所有数据库头发模型而定义的边界框界定。然后在边界框（H×H×D）内细分分辨率为128×128×96的3D体积。</p>
<p><strong>2D
捕捉</strong>：为了获得定义的模型空间下的二维信息图X，我们将相机直接放置在半身模型上。图像平面的中心与边界框的中心重合。二维图像是通过比例尺为
1024/H 的正交投影捕获的。因此，拍摄图像的尺寸为1024×1024。</p>
<p><strong>训练数据</strong></p>
<p>按照[2]，我们通过简单地翻转每个模型，将数据库的数量增加一倍，并删除辫子和辫子等受约束的发型。我们的数据库中有
303 种发型，从短到长，从直到卷。我们围绕边界框的中心随机旋转头发。
X轴旋转范围为-15o至15o，Y轴旋转范围为-30o至30o，Z轴旋转范围为-20o至20o。由于所有这些数据库模型都是由多边形带组成，与
Hu 等人相同。 [2]，我们将多边形条转换为密集的 3D 方向体积，视为地面实况
Y，然后生长线。然后，我们以第 4.1 节中定义的相机视图姿势将发丝渲染到 2D
图像。然而，为了消除真实头发图像和合成头发图像的差异，我们使用[16]的迭代方法计算捕获图像的二维方向和置信度图。考虑到真实图像质量的多样性，数据库的迭代随机范围为
3 到 5。通常，方向图和 Chai 等人中存在方向模糊性。
[17]已经证实，应该消除方向模糊性以确保毛发生长的正确方向。我们可以将模型链方向投影到图像平面来更新方向图以避免歧义。然后，我们以高置信度扩散方向以获得最终的像素密集方向图，并将方向向量编码到颜色空间。此外，如前所述，胸围模型也应该被视为我们网络的一个条件，因为头发是从头皮长出来并分布在身体各处。我们通过逐像素光线追踪来计算半身深度图，得到半身到相机的距离，并将距离除以
D，将值范围限制在 [0, 1] 范围内。最后生成由 2D
方向图、置信度图和半身深度图组成的网络输入 X。</p>
<p><img src="/pic/hairgans/2.png" /></p>
<p>所有 2D 映射的值都在 [0, 1] 范围内，3D 和 2D
方向向量都在颜色空间中编码。对于每个数据库模型，我们计算 12 对 X 和
Y。因此，我们得到 3636 对训练数据。训练数据生成示例如图3所示</p>
<h3 id="hairgans">HairGans</h3>
<p>通过从输入图像中提取的 2D 贴图和胸围深度图，我们的 Hair-GAN
的目标是生成一个 3D
方向体积，对占用和方向信息进行编码，以指导头发合成。网络的输入是大小为1024×1024的2D张量X，由统一模型空间中捕获的4个特征通道组成：头发方向图（编码为RG颜色的2D方向向量XY）、置信度图（置信度值作为灰色）和半身深度图（深度值作为灰色）。输出是大小为
128 × 128 × 96 的 3D 张量 Y，其中头发方向向量以 RGB
颜色编码。我们首先描述我们的对抗训练网络的损失函数（§5.1）。接下来，我们描述
Hair-GAN 的架构（§5.2 ）和训练策略（§5.3 ）。</p>
<p><strong>Loss Function</strong></p>
<p><strong>WGAN</strong>：在面对最优Discriminator时，GAN的Generator的优化目标可以写成JS散度的形式：
<span class="math display">\[
C(G)=-\log(4)+2\cdot JSD\left(p_\text{data}\left\|p_g\right.\right)
\]</span>
有关JS散度的目标函数会带来<strong>梯度消失</strong>的问题。如果Discriminator训练得太好，Generator就无法得到足够的梯度继续优化，而如果Discriminator训练得太弱，指示作用不显著，同样不能让Generator进行有效的学习。改进后的<span
class="math inline">\(-logD\)</span>
trick虽然解决了梯度消失的问题，然而又带来了mode
collapse、梯度不稳定等问题，同样存在理论缺陷。</p>
<p>WGAN认为需要对“生成分布与真实分布之间的距离”探索一种更合适的度量方法。作者们把眼光转向了<strong>Earth-Mover</strong>距离，简称<strong>EM</strong>距离，又称<strong>Wasserstein</strong>距离。</p>
<p>EM距离的定义为： <span class="math display">\[
W(P_r,P_g)=\inf_{\gamma\sim\Pi(P_r,P_g)}\mathbb{E}_{(x,y)\sim\gamma}[||x-y||]
\]</span> 其中，<span class="math inline">\(\Pi(P_r,P_g)\)</span>是<span
class="math inline">\(P_r\)</span>和<span
class="math inline">\(P_g\)</span>组合起来的所有可能的<strong>联合分布</strong>的集合，对于每一个可能的联合分布<span
class="math inline">\(\gamma\)</span>从中采样$(x,y)<span
class="math inline">\(，可以得到一个真实样本x和生成样本y，并可以计算样本距离\)</span>||x-y||<span
class="math inline">\(，所以可以计算该联合分布下**样本对距离**的期望值\)</span>_{(x,y)}[||x-y||]$，在<strong>所有可能的联合分布中</strong>能够对这个期望值取到的下界，就定义为EM距离。</p>
<p>作者们自然想到把EM距离用到GAN中。直接求解EM距离是很难做到的，不过可以用一个叫<strong>Kantorovich-Rubinstein
duality</strong>的理论把问题转化为： <span class="math display">\[
\begin{aligned}W(\mathbb{P}_r,\mathbb{P}_\theta)=\sup_{\|f\|_L\leq1}\mathbb{E}_{x\sim\mathbb{P}_r}[f(x)]-\mathbb{E}_{x\sim\mathbb{P}_\theta}[f(x)]\end{aligned}
\]</span>
这个公式的意思是对所有满足<strong>1-Lipschitz</strong>限制的函数<em>f</em>取到
<span class="math display">\[
\mathbb{E}_{x\sim\mathbb{P}_r}[f(x)]-\mathbb{E}_{x\sim\mathbb{P}_\theta}[f(x)]
\]</span>
的上界。简单地说，Lipschitz限制规定了一个连续函数的最大局部变动幅度，如K-Lipschitz就是：
<span class="math display">\[
|f(x_1)-f(x_2)|\leq K|x_1-x_2|
\]</span> 然后可以用神经网络的方法来解决上述优化问题： <span
class="math display">\[
\max_{w\in\mathcal{W}}\mathbb{E}_{x\sim\mathbb{P}_r}[f_w(x)]-\mathbb{E}_{z\sim
p(z)}[f_w(g_\theta(z)]
\]</span> 这里的 <span class="math display">\[
\{f_w\}_{w\in\mathcal{W}}
\]</span>
就是要优化的参数。这个神经网络和GAN中的Discriminator非常相似，只存在一些细微的差异，作者把它命名为<strong>Critic</strong>以便与Discriminator作区分。</p>
<p>WGAN的几大优越之处：</p>
<ul>
<li>不再需要纠结如何平衡Generator和Discriminator的训练程度，大大提高了GAN训练的稳定性：Critic（Discriminator）训练得越好，对提升Generator就越有利。</li>
<li>即使网络结构设计得比较简陋，WGAN也能展现出良好的性能，包括避免了mode
collapse的现象，体现了出色的鲁棒性。</li>
<li>Critic的loss很准确地反映了Generator生成样本的质量，因此可以作为展现GAN训练进度的定性指标。</li>
</ul>
<p><strong>WGAN-GP</strong>：作者们发现WGAN有时候也会伴随样本质量低、难以收敛等问题。WGAN为了保证Lipschitz限制，采用了weight
clipping的方法，然而这样的方式可能过于简单粗暴了，因此他们认为这是上述问题的罪魁祸首。具体而言，他们通过简单的实验，发现weight
clipping会导致两大问题：模型建模能力弱化，以及梯度爆炸或消失。他们提出的替代方案是给Critic
loss加入<strong>gradient penalty
(GP)</strong>，这样，新的网络模型就叫<strong>WGAN-GP</strong>。 <span
class="math display">\[
L=\mathbb{E}_{\tilde{\boldsymbol{x}}\sim\mathbb{P}_g}\left[D(\tilde{x})\right]-\mathbb{E}_{\boldsymbol{x}\sim\mathbb{P}_r}\left[D(x)\right]+\lambda\mathbb{E}_{\hat{\boldsymbol{x}}\sim\mathbb{P}_{\hat{\boldsymbol{x}}}}\left[(\|\nabla_{\hat{\boldsymbol{x}}}D(\hat{\boldsymbol{x}})\|_2-1)^2\right]
\]</span> GP项的设计逻辑是：当且仅当一个可微函数的梯度范数（gradient
norm）在任意处都不超过1时，该函数满足1-Lipschitz条件。至于为什么限制Critic的梯度范数趋向1（two-sided
penalty）而不是小于1（one-sided
penalty），作者给出的解释是，从理论上最优Critic的梯度范数应当处处接近1，对Lipschitz条件的影响不大，同时从实验中发现two-sided
penalty效果比one-sided penalty略好。</p>
<p>另一个值得注意的地方是，用于计算GP的样本<span
class="math inline">\(\hat{x}\)</span>是生成样本和真实样本的线性插值。</p>
<p><strong>回到HairGANs</strong></p>
<p>对于我们的例子，目标是训练一个生成器 G(X )，将输入 2D
张量映射到所需的输出 3D 张量<span
class="math inline">\(\widetilde{\mathcal{Y}}:\widetilde{\mathcal{Y}}=\bar{G(\mathcal{X})}\)</span>。同时，<strong>鉴别器通过条件潜在投影（conditional
latent projection）<span class="math inline">\(P(\mathcal{X})\)</span>
最大化 [?]<span
class="math inline">\(G(\mathcal{X})\)</span>的生成器分布与 <span
class="math inline">\(\mathcal{Y}\)</span> 的目标分布之间的
Wasserstein-1 距离</strong>。</p>
<p><strong>辨别器</strong>：最小化损失函数： <span
class="math display">\[
\begin{aligned}
L_{D}=&amp;
E[D(\widetilde{\mathcal{Y}},P(\mathcal{X}))]-E[D(\mathcal{Y},P(\mathcal{X}))]  
+\lambda
E[(\|\nabla_{\hat{\mathcal{Y}}}D(\hat{\mathcal{Y}},P(\mathcal{X}))\|_{2}-1)^{2}]
\end{aligned}
\]</span>
[?]这里的D是表示什么？作为discriminator的话为什么有两个参数</p>
<p>其中，第三项是随机变量<span
class="math inline">\(\hat{\mathcal{Y}}\)</span>的梯度惩罚，其中<span
class="math inline">\(\hat{\mathcal{Y}}\leftarrow\epsilon\mathcal{Y}+(1-\epsilon)\mathcal{Y}\)</span></p>
<p><strong>生成器</strong>：最小化损失函数： <span
class="math display">\[
L_G=-E[D(\widetilde{\mathcal{Y}},P(\mathcal{X}))]
\]</span>
然而，在我们的实验中，我们发现这个函数不能很好地优化生成器，因为真假之间的分布差异不能轻易地通过正负号来确定。受到之前工作
[25] 的启发，他们使用预训练网络 VGG
的选定层作为特征表示，将纹理风格从源图像转移到目标图像。</p>
<p>[25]：Image style transfer using convolutional neural networks
图像风格迁移。Deep
CNN已经能制造强大的电脑视觉系统可以提取图片高级别的含义。在VGG中，使用了3个3x3卷积核来代替7x7卷积核，使用了2个3x3卷积核来代替5*5卷积核，这样做的主要目的是在保证具有相同感知野的条件下，提升了网络的深度，在一定程度上提升了神经网络的效果。</p>
<p>在这里，我们在我们的研究中引入了风格和内容的损失，其中这些特征在选定的<strong>鉴别器层</strong>的域中表示。因此，优化生成器的目标是最小化能量：
<span class="math display">\[
\begin{aligned}
L_{G}^{*}&amp; =\alpha L_{content}+\beta L_{style}  
=\alpha\sum L_{content}^l+\beta\sum L_{style}^l \\
\end{aligned}
\]</span> 如文献[ 25 ]所述，内容损失由特征表示之间的误差平方损失来定义:
<span class="math display">\[
L_{content}^l=\frac12\sum_{ik}[f_{ik}^l(\mathcal{Y},P(\mathcal{X}))-f_{ik}^l(\widetilde{\mathcal{Y}},P(\mathcal{X}))]^2
\]</span>
其中l为选择层，i为第i个特征图，k为特征张量中的索引，f为判别器特征(以图4表示)。风格损失由Gram矩阵之间的均方距离定义，其中每个元素通过向量化特征图i和j之间的内积计算：$A_{ij}<sup>l=<em>kf</em>{ik}</sup>lf_{jk}^l。目标是：
<span class="math display">\[
L_{style}^l=\frac{1}{4N_l^2M_l^2}\sum_{ij}[A_{ij}^l(\mathcal{Y},P(\mathcal{X}))-A_{ij}^l(\widetilde{\mathcal{Y}},P(\mathcal{X}))]^2
\]</span> 式中：Nl为特征图的个数，Ml为特征张量(如l = 0 , N0 = 3 , M0 =
128 × 128 × 96)的大小。</p>
<h3 id="训练框架">训练框架</h3>
<ul>
<li>in(resolution, feature channels)</li>
<li>out(resolution, feature channels)</li>
<li>C(input channels, output channels, strides)</li>
<li><span class="math inline">\(\overline{\omega}\)</span> is
dimensional expansion layer</li>
<li>ζ is fully connected node</li>
<li>use + as the element-wise addition in the residual blocks
constituted by C</li>
<li>I denotes the input tensor current layer</li>
<li>对于所有2D卷积层C2，滤波器大小为5</li>
<li>对于所有3D卷积层C3，滤波器大小为3</li>
<li>X -、Y -、Z - info的运算块具有相同的方案形式</li>
</ul>
<p>生成器：我们将生成器举例说明为一些块。第一块以X为输入，由4个残差网络[
26
]组成，从前层到后层逐元素添加激活，以得到从高层到低层信息的残差修正，降采样后的特征映射为潜码，从1024
× 1024到128 × 128，特征数量从4个增加到256个。然后，X -，Y -和Z
-块分别将潜在代码编码为通道数为96的特征，这些特征在结果卷中沿Z轴的分辨率。$将2D特征的继承转换为3D特征的单个通道。然后，我们将X
-，Y -，Z
-块的输出作为输入串联到下面的3D残差卷积网络中。具体见图4和表1。</p>
<p>判别器。考虑到2D输入X与3D期望输出" Y / Y "之间的对应关系，受文献[ 20
]的启发，我们将" Y / Y "与编码X的特征图P ( X )串联到与" Y / Y
"具有相同分辨率的3D隐空间，然后将串联后的3D特征张量通过多个滤波器进行卷积，直到ζ层，最终区分真假。</p>
<h3 id="相关工作">相关工作</h3>
<p>头发是网络游戏、虚拟世界和虚拟现实应用中数字角色的重要组成部分之一。高质量
3D
头发建模技术在计算机图形学中得到了广泛的研究，这通常需要专业技能和数天的艰苦手工工作。关于精毛建模方法的详细讨论请参阅调查[15]。</p>
<p>基于图像的头发建模是一种很有前途的方法，可以根据捕获的头发图像创建引人注目的头发结构。根据所需图像的数量，基于图像的头发建模方法可以大致分为多视图头发建模和单视图头发建模。多视图头发建模方法
[4] [5] [6] [7] [8] [9] 从多个视图创建高质量的 3D
头发建模，这通常需要复杂的硬件设置、良好控制的环境，加工周期长。它们对消费者不友好，因为普通用户不容易掌握这些多视图捕获系统和专业技能。虽然单视图头发建模方法与单视图一样变得越来越流行和重要，但未经校准的图像在互联网上广泛存在。柴等人。
[16][17]首先介绍了利用不同类型的先验知识（包括层边界和遮挡以及阴影线索）进行单视图头发建模的技术[3]。他们的方法的一个主要问题是缺乏对远离输入图像的视图中的几何形状的控制。</p>
<p>通过 3D
合成发型数据库提供了整个发型的概念上有说服力的先验知识。胡等人。
[2]通过对数据库中搜索到的不同发型进行组合，通过与用户的几次笔画进行拟合来重建完整的头发形状。柴等人。
[1]将模型重新混合步骤提出到预计算阶段。从扩大的数据库中找到大约 5-40
个候选者，然后对这些候选者进行变形，以获得具有细节相似性的模型结果。为了将
3D 头发数据库丰富到 40K
模型，他们对发丝进行聚类并重新组合这些聚类模型。张等人。
[18]仅使用轮廓拟合搜索到的候选者进行基于四视图的头发建模来构建平滑的粗糙头发形状，并通过纹理融合和螺旋拟合引入风格细节。在[19]中，他们引入了一种基于局部补丁的搜索策略来寻找具有足以指导头发合成的局部风格模式的候选者，而不是在全局中寻找具有相同风格的候选者。所有这些数据驱动的方法都需要存储数百或数千个发型数据库</p>
<p>深度学习最近的成功也给头发建模领域带来了显着的进步。柴等人。
[1]提出了一种全自动头发建模方法，通过用深度卷积神经网络代替用户交互来进行头发分割和头发生长方向估计。胡等人。
[10]引入基于深度学习的头发属性分类器来提高数据驱动方法的候选检索性能。为了获得从
2D 图像知识到 3D 头发表示的端到端学习，Zhou 等人。 [11]
使用编码器-解码器架构生成表示为 3D 点序列的发丝，以 2D
方向场作为输入。但他们的头发表示被参数化为头皮上的低分辨率网格，这导致建模结果质量较低。在同时进行的工作中，Saito
等人。 [12] 证明了 3D
占用场和相应的高分辨率流场很容易通过神经网络处理，并且与传统的基于链的表示兼容，用于高保真建模和渲染。然而，在他们的方法中，占用场和流场是从同一体积潜在空间分开解码的。他们对
ResNet-50
的预训练网络进行微调，将输入图像编码为头发系数，这些系数通过训练好的嵌入网络与体积潜在空间对齐。由于编码器处理过程中存在压缩以及潜在系数对齐中的信息丢失，因此他们的结果缺乏输入图像和输出头发结构之间的对应细节。相比之下，以
2D 信息图作为输入，我们的方法更直接地训练 Hair-GAN
来预测编码占用和方向信息的 3D
体积场，同时考虑输入图像和建模结果之间的细节对应关系。</p>
<p>Goodfellow 等人提出了生成对抗网络（GAN）。
[14]作为构建可以模仿目标分布的生成模型的框架。 GAN
的目标是通过依次迭代训练判别器和生成器来训练生成器模型。条件 GAN [20]
是一种使用条件信息作为判别器和生成器的
GAN，被视为图像领域很有前途的工具，例如条件图像合成 [21]、从文本生成图像
[22] 和图像到图像的翻译[23]。我们采用 GAN 从 2D 图像信息中恢复 3D
头发结构，利用 GAN
的强大功能来重新创建复杂数据集的分布。我们利用鉴别器的潜在空间来增强分布中的真实值和目标输出的相似性。我们的
Hair-GAN 旨在学习从 2D 信息图到 3D
体积占用和方向场的参数转换，没有中间潜在空间。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/07/13/GAN-reading-and-implementation/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Timako">
      <meta itemprop="description" content="Time heals">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Timako world">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2023/07/13/GAN-reading-and-implementation/" class="post-title-link" itemprop="url">GAN reading and implementation</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2023-07-13 13:25:47 / 修改时间：18:46:47" itemprop="dateCreated datePublished" datetime="2023-07-13T13:25:47+08:00">2023-07-13</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>生成对抗网络 (Generative Adversarial Network, GAN)
是一类神经网络，通过轮流训练判别器 (Discriminator) 和生成器
(Generator)，令其相互对抗，来从复杂概率分布中采样，例如生成图片、文字、语音等。今天我们开始阅读并且实现最简单的GAN网络。</p>
<h3 id="熵">熵</h3>
<p>熵的公式如下： <span class="math display">\[
Entropy=-\sum_i P(i)\log_2P(i)
\]</span> 其中，已知一个离散变量 i 的概率分布P(i)，<span
class="math inline">\(\log_2P(i)\)</span>为此时的最短编码长度，熵的含义即为平均编码长度。对于连续变量
x 的概率分布P(x)，熵的公式可以表示为： <span class="math display">\[
Entropy=-\int P(x)\log_2P(x)dx
\]</span></p>
<p>在熵的公式中，对于离散变量和连续变量，我们都是计算了 负的可能性的对数
的期望，代表了该事件理论上的平均最小编码长度，所以熵的公式也可表示如下，公式中的x~P代表我们使用概率分布P来计算期望，熵又可以简写为H：
<span class="math display">\[
H(P)=Entropy=\mathbb{E}_{x\sim P}[-\log P(x)]
\]</span></p>
<h3 id="交叉熵">交叉熵</h3>
<p>假如我们现在需要预报东京天气，在真实天气发生之前，我们不可能知道天气的概率分布；但为了下文的讨论，我们需要假设：对东京天气做一段时间的观测后，可以得到真实的概率分布P（在机器学习模型中，对应标签的真实值）。在观测之前，我们只有预估的概率分布Q（在机器学习模型中，对应预测器），使用估计得到的概率分布，可以计算估计的熵：
<span class="math display">\[
EstimatedEntropy=\mathbb{E}_{x\sim Q}\left[-\log Q(x)\right]
\]</span>
和香农的目标一样，我们希望编码长度尽可能的短，所以我们需要对比我们的编码长度和理论上的最小编码长度(熵)。假设经过观测后，我们得到了真实概率分布P，在天气预报时，就可以使用P计算平均编码长度，实际编码长度基于Q计算，这个计算结果就是P和Q的交叉熵。这样，实际编码长度和理论最小编码长度就有了对比的意义。
<span class="math display">\[
CrossEntropy=\mathbb{E}_{x\sim P}[-\log Q(x)]
\]</span>
交叉熵使用H(P,Q)表示，意味着使用P计算期望，使用Q计算编码长度；所以H(P,Q)并不一定等于H(Q,P)，除了在P=Q的情况下，H(P,Q)
= H(Q,P) =
H(P)。对于编码长度，我们使用假设的概率分布Q来计算，因为它是预估用于编码信息的。因为熵是理论上的平均最小编码长度，所以交叉熵只可能大于等于熵。换句话说，如果我们的估计是完美的，即Q=P，那么有H(P,Q)
= H(P)，否则，H(P,Q) &gt; H(P)。由于<span class="math inline">\(-\log
Q(x) \ge 0\)</span>，熵的值只可能大于等于0，当且仅当 ： <span
class="math display">\[
\begin{cases}P(i)=1,i=i_k\\P(i)=0,i\neq i_{k}\end{cases}
\]</span> 时，熵为0。交叉熵越小，代表预测结果越准确。</p>
<h3 id="多分类交叉熵">多分类交叉熵</h3>
<p>假设一个动物照片的数据集中有5种动物，且每张照片中只有一只动物，每张照片的标签都是one-hot编码。</p>
<table>
<thead>
<tr class="header">
<th>Fox</th>
<th>Cat</th>
<th>Dog</th>
<th>Rabbit</th>
<th>Pig</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>[1,0,0,0,0]</td>
<td>[0,1,0,0,0]</td>
<td>[0,0,1,0,0]</td>
<td>[0,0,0,1,0]</td>
<td>[0,0,0,0,1]</td>
</tr>
</tbody>
</table>
<p>我们的两个预测模型对于Fox的图片预测结果如下：</p>
<table>
<thead>
<tr class="header">
<th>Model</th>
<th>Prediction</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Q1</td>
<td>[0.4, 0.3, 0.05, 0.05, 0.2]</td>
</tr>
<tr class="even">
<td>Q2</td>
<td>[0.98, 0.01, 0, 0, 0.01]</td>
</tr>
</tbody>
</table>
<p>两个模型交叉熵计算结果如下： <span class="math display">\[
\begin{aligned}H(P_1,Q_1)&amp;=-\sum_iP_1(i)\log_2Q_1(i)\\&amp;=-(1\log0.4+0\log0.3+0\log0.05+0\log0.05+0\log0.2)\approx0.916\end{aligned}
\]</span></p>
<p><span class="math display">\[
\begin{aligned}H(P_1,Q_2)&amp;=-\sum_iP_1(i)\log_2Q_2(i)\\&amp;=-(1\log0.98+0\log0.01+0\log0+0\log0+0\log0.01)\approx0.02\end{aligned}
\]</span></p>
<p>交叉熵对比了模型的预测结果和数据的真实标签，随着预测越来越准确，交叉熵的值越来越小，如果预测完全正确，交叉熵的值就为0。</p>
<h3 id="二分类交叉熵">二分类交叉熵</h3>
<p>在二分类模型中，标签只有是和否两种；这时，可以使用二分类交叉熵作为损失函数。假设数据集中只有猫和狗的照片，则交叉熵公式中只包含两种可能性：
<span class="math display">\[
\begin{gathered}
\begin{aligned}H(P,Q)=-\sum_{i=(cat,dog)}P(i)\log Q(i)\end{aligned} \\
=-P(cat)\log Q(cat)-P(dog)\log Q(dog)
\end{gathered}
\]</span></p>
<p><span class="math display">\[
H(P,Q)=-P(cat)\log Q(cat)-\text{(}1-P(cat))\log\left(1-Q(cat)\right)
\]</span></p>
<p>如果采用以下定义： <span class="math display">\[
\begin{aligned}P&amp;=P(cat)\\\tilde{P}&amp;=Q(cat)\end{aligned}
\]</span> 那么交叉熵可以写成这种形式： <span class="math display">\[
BinaryCrossEntropy=-P\log\tilde{P}-\text{(}1-P)\log{(1-\tilde{P})}
\]</span></p>
<h3 id="生成器-判别器模型">生成器-判别器模型</h3>
<p>初始化判别器D的参数 <span
class="math inline">\(\theta_d\)</span>和生成器G的参数<span
class="math inline">\(\theta_g\)</span></p>
<p><strong>循环k次更新判别器之后，使用较小的学习率来更新一次生成器的参数</strong>，训练生成器使其尽可能能够减小生成样本与真实样本之间的差距，也相当于尽量使得判别器判别错误。
从真实样本中采样m个样本 { <span class="math inline">\(x^1,x^2,\ldots
x^m\)</span> } ，从先验分布噪声中采样 m 个噪声样本 { <span
class="math inline">\(z^1,z^2,\ldots,z^m\)</span> } 并通过生成器获取 m
个生成样本 { <span
class="math inline">\(\tilde{x}^1,\tilde{x}^2,\ldots,\tilde{x}^m\)</span>
}
。固定生成器G，训练判别器D尽可能好地准确判别真实样本和生成样本，尽可能大地区分正确样本和生成的样本。</p>
<p>多次更新迭代之后，最终理想情况是使得判别器判别不出样本来自于生成器的输出还是真实的输出。亦即最终样本判别概率均为0.5。</p>
<p>在GAN的模型下，判别器是一个二分类问题，交叉熵可以定义为： <span
class="math display">\[
H((x_1,y_1),D)=-y_1\log D(x_1)-(1-y_1)\log(1-D(x_1))
\]</span> 其中，假定 <span
class="math inline">\(y_1\)</span>为正确样本分布，那么对应的<span
class="math inline">\((1-y_1)\)</span>就是生成样本（因为在这种情况下，生成的不是真实图片，所以判别器应该将其判定为错）的分布。
<span class="math inline">\(D\)</span>表示判别器，则 <span
class="math inline">\(D(x_1)\)</span> 表示判别样本为正确的概率， <span
class="math inline">\(1-D(x_1)\)</span>
则对应着判别为错误样本的概率。这里仅仅是对当前情况下的交叉熵损失的具体化。</p>
<p>将上式推广到N个样本后，将N个样本相加得到对应的公式如下： <span
class="math display">\[
H((x_{i},y_{i})_{i=1}^{N},D)=-\sum_{i=1}^{N}y_{i}\operatorname{log}D(x_{i})-\sum_{i=1}^{N}(1-y_{i})\operatorname{log}(1-D(x_{i}))
\]</span> 对于GAN中的样本点 <span
class="math display">\[x_i\]</span>对应于两个出处,要么来自于真实样本,要么来自于生成器生成的样本<span
class="math inline">\(\tilde{x}\sim G(z)\)</span>这里的<span
class="math inline">\(z\)</span>服从于投到生成器中噪声的分布。生成器试图将以下函数最小化，而判别器则尝试将其最大化：
<span class="math display">\[
E_x[log(D(x))]+E_z[log(1-D(G(z)))]
\]</span> 将最小化-最大化的过程放在公式里，可得： <span
class="math display">\[
\min_G\max_DV(D,G)=\mathbb{E}_{\boldsymbol{x}\sim
p_{\dim}(\boldsymbol{x})}[\log
D(\boldsymbol{x})]+\mathbb{E}_{\boldsymbol{z}\sim
p_{\boldsymbol{z}}(\boldsymbol{z})}[\log(1-D(G(\boldsymbol{z})))].
\]</span></p>
<p>在此函数中：</p>
<ul>
<li><code>D(x)</code> 是判别器对真实数据实例 x 真实的概率的估计值。</li>
<li>Ex 是所有真实数据实例的预期值。</li>
<li><code>G(z)</code> 是指定的噪声 z 时的输出。</li>
<li><code>D(G(z))</code> 是判别器对虚构实例是真实概率的估计值。</li>
<li>Ez 是指向生成器的所有随机输入的预期值（实际上是所有生成的虚构实例
G(z) 的预期值）。</li>
<li>该公式由真实分布和生成的分布之间的<a
target="_blank" rel="noopener" href="https://developers.google.com/machine-learning/glossary?hl=zh-cn#cross-entropy">交叉熵</a>得出。</li>
</ul>
<p>生成器不能直接影响函数中的 <code>log(D(x))</code>
项，因此对于生成器，最大限度降低损失相当于最小化
<code>log(1 - D(G(z)))</code>。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/07/12/Record-Ubuntu22-04-installation/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Timako">
      <meta itemprop="description" content="Time heals">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Timako world">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2023/07/12/Record-Ubuntu22-04-installation/" class="post-title-link" itemprop="url">Record Ubuntu22.04 installation</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2023-07-12 22:49:38" itemprop="dateCreated datePublished" datetime="2023-07-12T22:49:38+08:00">2023-07-12</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2023-07-13 10:40:00" itemprop="dateModified" datetime="2023-07-13T10:40:00+08:00">2023-07-13</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>又重装了一次系统，这次重装的是Ubuntu 22.04
LTS，记录一下重装系统+系统配置的过程</p>
<p>分区之前的就不记录了，分区时提示“No EFI ...”，分配了1G
<code>EFI</code>，500M
<code>/boot</code>（虽然可能用不到，但是网上的教程比较杂乱），剩下的空间全部分配给<code>/</code></p>
<p>分区结束拔掉u盘重新启动，配置网络，选择ZJUWLAN-Secure，网络类型TUNNELLD
TLS，选中 <code>No CA Certificate</code>，输入账户密码</p>
<p>把<code>clash-linux-amd64</code>复制到系统上启动，初始化<code>.config/clash</code>文件夹，把配置文件复制进来</p>
<p>配置代理，设置<code>Manual</code>，HTTP Proxy和HTTPS
Proxy都填写<code>127.0.0.1:7890</code>，Socks
Host选择填写<code>127.0.0.1:7891</code>，搜索Startup，把clash设置成开机启动项。</p>
<p>打开<code>Displays</code>设置双屏</p>
<p>去https://mirrors.tuna.tsinghua.edu.cn/help/ubuntu/设置清华源</p>
<p>安装anaconda</p>
<p>安装nvidia显卡驱动
https://blog.csdn.net/Perfect886/article/details/119109380</p>
<p>安装cuda
https://blog.csdn.net/qq_37041791/article/details/126932883</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get update  </span><br><span class="line"> </span><br><span class="line">sudo apt-get install g++</span><br><span class="line"> </span><br><span class="line">sudo apt-get install gcc</span><br><span class="line"> </span><br><span class="line">sudo apt-get install make</span><br></pre></td></tr></table></figure>
<p>安装cudnn：默认安装即可</p>
<p>安装pytorch和torchvision：download.pytorch.org/whl/cu117下载torch
1.13.1 and torchvision 0.14.1，anaconda创建新环境，pip install安装</p>
<p>最后下载vscode，测试pytorch是否可以使用cuda</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line"></span><br><span class="line"># setting device on GPU if available, else CPU</span><br><span class="line">device = torch.device(&#x27;cuda&#x27; if torch.cuda.is_available() else &#x27;cpu&#x27;)</span><br><span class="line">print(&#x27;Using device:&#x27;, device)</span><br><span class="line">print()</span><br><span class="line"></span><br><span class="line">#Additional Info when using cuda</span><br><span class="line">if device.type == &#x27;cuda&#x27;:</span><br><span class="line">    print(torch.cuda.get_device_name(0))</span><br><span class="line">    print(&#x27;Memory Usage:&#x27;)</span><br><span class="line">    print(&#x27;Allocated:&#x27;, round(torch.cuda.memory_allocated(0)/1024**3,1), &#x27;GB&#x27;)</span><br><span class="line">    print(&#x27;Cached:   &#x27;, round(torch.cuda.memory_reserved(0)/1024**3,1), &#x27;GB&#x27;)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>到此完成</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/07/11/HRRVC-paper-reading/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Timako">
      <meta itemprop="description" content="Time heals">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Timako world">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2023/07/11/HRRVC-paper-reading/" class="post-title-link" itemprop="url">HRRVC paper reading</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2023-07-11 16:18:08 / 修改时间：18:33:42" itemprop="dateCreated datePublished" datetime="2023-07-11T16:18:08+08:00">2023-07-11</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="符号表">符号表</h2>
<table>
<colgroup>
<col style="width: 47%" />
<col style="width: 20%" />
<col style="width: 31%" />
</colgroup>
<thead>
<tr class="header">
<th>符号</th>
<th>域</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(y\)</span></td>
<td><span class="math inline">\(\mathbb{R}^3\)</span></td>
<td>光顶点</td>
</tr>
<tr class="even">
<td><span class="math inline">\(z\)</span></td>
<td><span class="math inline">\(\mathbb{R}^3\)</span></td>
<td>眼顶点</td>
</tr>
<tr class="odd">
<td><span class="math inline">\(P(y,z)\)</span></td>
<td><span class="math inline">\([0,1]\)</span></td>
<td>俄罗斯轮盘的概率</td>
</tr>
<tr class="even">
<td><span class="math inline">\(n\)</span></td>
<td><span class="math inline">\(S^2\)</span></td>
<td>表面法线</td>
</tr>
<tr class="odd">
<td><span class="math inline">\(\omega\)</span></td>
<td><span class="math inline">\(S^2\)</span></td>
<td>散射波瓣上的一个方向</td>
</tr>
<tr class="even">
<td><span class="math inline">\(\omega &#39;\)</span></td>
<td><span class="math inline">\(S^2\)</span></td>
<td>入射方向</td>
</tr>
<tr class="odd">
<td><span class="math inline">\(\rho(z, \omega,
\omega&#39;)\)</span></td>
<td><span class="math inline">\([0,\infty]\)</span></td>
<td>z处的BRDF</td>
</tr>
<tr class="even">
<td><span class="math inline">\(q_z(\omega)\)</span></td>
<td><span class="math inline">\([0,\infty]\)</span></td>
<td>z处的近似散射波瓣</td>
</tr>
<tr class="odd">
<td>$_i $</td>
<td><span class="math inline">\([0,1]\)</span></td>
<td>第i个均匀随机数</td>
</tr>
<tr class="even">
<td><span class="math inline">\(R(\omega;z,\xi_i)\)</span></td>
<td><span class="math inline">\([0,\infty]\)</span></td>
<td>z处的光顶点接受范围</td>
</tr>
<tr class="odd">
<td><span class="math inline">\(D(·)\)</span></td>
<td><span class="math inline">\([0,\infty]\)</span></td>
<td>GGX分布</td>
</tr>
<tr class="even">
<td><span class="math inline">\(K(·)\)</span></td>
<td><span class="math inline">\([0,1]\)</span></td>
<td>平方椭球波瓣(SEL)函数</td>
</tr>
<tr class="odd">
<td><span class="math inline">\(\alpha_x, \alpha_y\)</span></td>
<td><span class="math inline">\([0,1]\)</span></td>
<td>GGX的粗糙度</td>
</tr>
<tr class="even">
<td><span class="math inline">\(\hat{\alpha}_x,
\hat{\alpha}_y\)</span></td>
<td><span class="math inline">\([0,1]\)</span></td>
<td>GGX的粗糙度</td>
</tr>
<tr class="odd">
<td><span class="math inline">\(M\)</span></td>
<td><span class="math inline">\(\mathbb{N}\)</span></td>
<td>光路径数量</td>
</tr>
</tbody>
</table>
<h2 id="层次化的俄罗斯轮盘赌">层次化的俄罗斯轮盘赌</h2>
<h3 id="随机散射范围">随机散射范围</h3>
<p>对于光顶点y和眼顶点z的连接，俄罗斯轮盘赌的接受概率： <span
class="math display">\[
P(\mathbf{y},\mathbf{z})=\min\left(\frac{Cq_z\left(\overrightarrow{\mathbf{zy}}\right)}{\|\mathbf{y}-\mathbf{z}\|^2},1\right)
\]</span> 其中，<span class="math inline">\(C \in [0,\infty)\)</span>
是一个用户指定的参数，可用来控制方差和性能之间的权衡。球函数<span
class="math inline">\(q_z\)</span>约等于散射波瓣： <span
class="math display">\[
\operatorname{q}_{\mathrm{z}}\left(\omega\right)\approx\operatorname{p}\left(\mathrm{z},\omega&#39;,\omega\right)\left|\omega\cdot\mathrm{n}\right|
\]</span> 令 ξi 为第 i 个均匀随机数，则从 z
开始的世界空间接受范围的边界是一个球函数，由满足 P(y, z) = ξi 的距离 ∥y
− z∥ 给出(只有在这个范围内的光顶点才会和眼顶点z连接)。</p>
<p>wyd: 注意P(y, z) = ξi
表示俄罗斯轮盘赌的接受概率，该随机数越小，说明越不可能接受俄罗斯轮盘赌，也就是说这个节点内的光顶点越不可忽略，所以此处的世界空间接受范围的边界越大
<span class="math display">\[
R(\omega;z,\xi_i)=\sqrt{\frac{Cq_z(\omega)}{\xi_i}}.
\]</span></p>
<h3 id="层次化拒绝">层次化拒绝</h3>
<p>为了快速地在散射范围<span
class="math inline">\(\operatorname{R}(\omega;z,\xi_i)\)</span>中寻找光顶点，我们构建了一个光顶点的二分层级包围体(BVH)。使用这个BVH，散射范围和光顶点的包围盒之间的相交测试就可以使用一种自上而下的分层方式进行。然而，尽管散射范围的形状只依赖于近似波瓣<span
class="math inline">\(q_z(\omega)\)</span>,
但是散射范围的大小却依赖于随机变量<span
class="math inline">\(\sqrt{C/\xi_i}\)</span>
。如果按照论文中的说法来说，俄罗斯轮盘赌的接受概率和光顶点和眼顶点都有关，因此要对每一个光顶点生成这样的随机数，这显然是不可接受的。因此对于每个BVH节点执行相交测试，我们需要得到<span
class="math inline">\(\sqrt{\operatorname{C}/\min_{i\in\mathrm{L}}\xi_i}\)</span>以确定散射范围，其中其中<span
class="math inline">\(L\)</span>表示该节点包含的叶子节点的索引集。所以对于每一个BVH，我们使用自上而下生成这个随机数的方法</p>
<h4 id="预生成随机数的问题">预生成随机数的问题</h4>
<p>一种低效的获取每一个节点中最小的随机数的方法就是以自下而上的方式进行预计算。首先，在预处理步骤中将单个随机数分配给每个光顶点，其方法类似于随机光剔除方法。然后，在构建BVH的阶段，将最小随机数从叶子节点传播到更上层的节点。然而，因为对于所有的眼睛顶点都是使用相同的预计算的随机数，这种方法就会在眼睛顶点间产生相关性方差。如果眼睛顶点被密集采样(例如超采样)的话，这种相关性方差会影响计算效率，并且在后处理去噪过程中(例如图片去噪)也会极大地降低效率。为了避免相关性方差，就必须为每一对光顶点和眼睛顶点赋予不同的随机数。但是，在每一个眼睛顶点处都进行一次完全自下而上传播的代价明显是非常昂贵的。我们提出了一个实时生成最小随机数的方法，该方法是自上而下的方式并且不会对所有光顶点生成随机数，自然也就不需要自下而上地传播随机数了。</p>
<h3 id="即时最小随机数生成">即时最小随机数生成</h3>
<p>我们首先讨论 O(1) 方法来生成均匀随机数中的最小值。
然后，我们将讨论扩展到自上而下的分层算法，为每个 BVH
节点生成最小随机数。为了提高该算法的数值稳定性，我们使用半分层抽样方法，该方法使用重叠层生成均匀且部分分层的随机数。</p>
<h4 id="最小随机数的-pdf">最小随机数的 PDF</h4>
<p>N 个均匀随机数中的最小值 {ξ1, . . . , ξN } 可以通过考虑其 PDF
生成。这个PDF为： <span class="math display">\[
P_{\min,N}(u)=N(1-u)^{N-1}
\]</span>
这个公式的含义是对于N个0~1的随机数，他们最小值为u的概率。又因为这个PDF的累积分布函数的逆函数也有一个封闭形式的解，所以说只要生成一个均匀的随机数，就能随机地生成一个N个随机数中的最小随机数
<span class="math display">\[
\min\{\xi_1,\dots,\xi_N\}=1-(1-\xi)^{\frac{1}{N}}.
\]</span> 尽管基于以上公式，我们可以使用一种实时的随机数生成方法，但当N
NN很大时会有精度问题(如图4b所示)。然而，对于1维分层采样的情况(因为是对随机数的边界进行分层，所以这里是1维)，我们可以避免数值误差并简化公式。由于分层随机数的最小值总是在最低层内，因此，可以简化为：
<span class="math display">\[
\min\{\xi_1,\dots,\xi_N\}=\frac{\xi}{N}.
\]</span> 而这个最小值的pdf为 <span
class="math inline">\(P_{\min,N}(u)=N\)</span> ,对于 <span
class="math inline">\(u\in[0,1/\mathrm{N}]\)</span>, 否则<span
class="math inline">\({p_{min,N}}\left(u\right)=0\)</span></p>
<h4 id="层次化生成bvh中的随机数">层次化生成bvh中的随机数</h4>
<p><img src="/pic/1.png" /></p>
<p>对于我们的BVH来说，父节点的最小随机数一定等于子节点最小随机数中的最小值。因此，在自上而下的分层生成过程中，父节点的最小随机数就会传播给一个子节点(蓝色节点)。然后，对于另一个节点(橙色节点)，就需要生成一个比其父节点值大的新的最小随机数。继承父节点最小随机数值的子节点是根据子节点中包含的叶子节点数量来随机选择的。这是因为如果所有叶子节点中的随机数是均匀分布的话，那么子树中出现最小值的概率与叶子节点数量成正比。因此，子节点被选中的概率为：
<span class="math display">\[
\mathrm P_{selection}(\mathrm j)=\frac{|\mathrm L_\mathrm j|}{|\mathrm
L_\mathrm j|+|\mathrm L_{\mathrm j&#39;}|}
\]</span> 轮盘赌自上而下传播的实际上是一个上确界b。参考下图算法：</p>
<p><img src="/pic/2.png" /></p>
<p>b是父节点的概率上确界，随后，向选中的l节点中传递这个概率上确界，同时计算一个属于[b,
1]的兄弟结点的概率上确界b'，传递给另一个未被选中的兄弟节点，而新的最小随机数只需要生成一个单一的均匀随机数就可获得，公式如下：
<span class="math display">\[
\min\limits_{\mathrm{i}\in\mathrm{L}}\xi_\mathrm{i}=\mathrm{b}+(1-\mathrm{b})\frac{\xi}{|\mathrm{L}|}
\]</span></p>
<h3 id="接受概率的各向异性波瓣近似">接受概率的各向异性波瓣近似</h3>
<h4 id="ggx分布">GGX分布</h4>
<p>SEL函数基于GGX分布</p>
<h4 id="平方球波瓣ssl">平方球波瓣(SSL)</h4>
<p>对于微平面BRDF，可用半角近似各项同性GGX分布的散射波瓣：</p>
<h4 id="平方椭球波瓣sel">平方椭球波瓣(SEL)</h4>
<p>各向同性的球函数，不能表示这种各向异性的散射效果。因此，我们将平方球波瓣推导到了平方椭球波瓣(SEL)</p>
<p><img src="/pic/3.png" /></p>
<h4 id="ggx的近似">GGX的近似</h4>
<p>尽管GGX是一个半球分布，但SEL是一个椭球函数。然而，对于低粗糙度的情况，可以使用下面的公式来近似
<span class="math display">\[
\pi\alpha_x\alpha_y
D\left(\omega;\mathbb{Q},\alpha_x,\alpha_y\right)\approx
K\left(\omega;\mathbb{Q},\frac{\alpha_x}{2},\frac{\alpha_y}{2}\right)
\]</span></p>
<h4 id="各向异性散射波瓣的近似">各向异性散射波瓣的近似</h4>
<p>对于使用了多重重要性采样(MIS)的双向路径跟踪(BPT)，只有在散射波瓣峰值的附近(即BRDF和cos的乘积)，PDF的近似准确率才是重要的。因此，我们在近似的峰值方向$
_z$ 使用原始的波瓣值来作为SEL的系数： <span class="math display">\[
q_z(\omega)=cK(\omega;Q,\dot{\alpha}_x,\dot{\alpha}_y)
\]</span> 其中<span
class="math inline">\(\text{c}=\rho\left(\text{z},\omega&#39;,\omega_\text{z}\right)\left|\omega_\text{z}\cdot\text{n}\right|\)</span>是有关BRDF的系数</p>
<h3 id="结合进双向路径追踪">结合进双向路径追踪</h3>
<p>本文结合了我们基于俄罗斯轮盘赌的分层连接和概率连接[Popov et al。
2015]（或常规顶点连接）通过使用多重重要性采样（MIS）。这是因为我们的连接对于镜面漫反射
(SDG) 或光泽-漫反射-光泽 (GDG)
路径是有效的，而概率连接对于低频照明效果是有效的。概率连接使用概率质量函数
(PMF)
从缓存中为给定的眼睛顶点采样重要的灯光顶点，该函数考虑了灯光和眼睛顶点的可见性、几何项和
BRDF。然而，概率连接必须将重用光子路径的数量限制在一个较小的数量（例如
100），因为它的计算开销和内存需求与重用子路径计数和 PMF
记录数的乘积成正比。对于极其光滑的表面，使用数百个样本仍然不够。虽然我们的方法的接受概率忽略了光顶点处的可见性和
BRDF，但重用数百万光子路径减轻了概率连接以及常规顶点连接的限制。</p>
<h4 id="多重重要性抽样">多重重要性抽样</h4>
<p>为了使用强大的 MIS 策略，例如平衡启发算法，必须获得每种技术的 [
样本计数和 PDF ] 的乘积。 M
光子路径的简单俄罗斯轮盘赌简单地给出了这个采样密度： <span
class="math display">\[
d_t(\bar{\mathbf{x}})=MP(\mathbf{y}_{s-1},\mathbf{z}_{t-1})p_t(\bar{\mathbf{x}}),
\]</span></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/07/11/VAE-paper-reading/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Timako">
      <meta itemprop="description" content="Time heals">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Timako world">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2023/07/11/VAE-paper-reading/" class="post-title-link" itemprop="url">VAE paper reading</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2023-07-11 16:14:52" itemprop="dateCreated datePublished" datetime="2023-07-11T16:14:52+08:00">2023-07-11</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2023-07-19 17:40:49" itemprop="dateModified" datetime="2023-07-19T17:40:49+08:00">2023-07-19</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>这是对VAE（变分编码器）论文的阅读和代码实现记录</p>
<h2 id="论文阅读">论文阅读</h2>
<h3 id="intuition">Intuition</h3>
<p><img src="/pic/vae/1.png" /></p>
<p>其中，x是输入变量，比如图像等，他是一个很高维度的随机变量；z是不可观测的（latent），一般维度比x低很多，用来描述“更本质的”关于x的信息。这里，我们假设z是满足一个分布<span
class="math inline">\(\mathcal{p}_\theta(z)\)</span>，而<span
class="math inline">\(\mathcal{p}_\theta(x|z)\)</span>表示从z采样一个x。<span
class="math inline">\(\theta\)</span>是一个参数，假设z满足一定的分布，所以也有从<span
class="math inline">\(\theta\)</span>到z的箭头。后面的星号<span
class="math inline">\(\theta\)</span>*表示ground truth，之后提及<span
class="math inline">\(\theta\)</span>都是指decoder里面的参数。x加上encoder里面的参数<span
class="math inline">\(\phi\)</span>就可以尝试推出z来。</p>
<p>举个例子：</p>
<p><img src="/pic/vae/2.png" /></p>
<p>左图是一个图像，图像里面有各种各样颜色，形状，大小不一样的图形，而右图是一些参数，通过更改这些参数，我们能够做到对图像的更改。</p>
<p>对于贝叶斯公式<span
class="math inline">\(p_{\boldsymbol{\theta}}(\mathbf{z}|\mathbf{x})=p_{\boldsymbol{\theta}}(\mathbf{x}|\mathbf{z})p_{\boldsymbol{\theta}}(\mathbf{z})/p_{\boldsymbol{\theta}}(\mathbf{x})\)</span>，我们至少需要知道右边的两个参数（剩下的先验概率可以估计着用）。然而，z的分布我们是不知道的，<span
class="math inline">\(\mathcal{p}_\theta(x)\)</span>是一个积分，然而事实上x和z都是很高维度的变量，积分实际上是无法进行的（非常高层的积分）。因此，从x推导到z，用纯计算的方法是很难进行的。VAE的motivation就是如此，他希望用一个神经网络来近似这一概率分布，在参数<span
class="math inline">\(\phi\)</span>的帮助下，能让q尽量接近p。而为了衡量这两个分布的相似程度，我们就自然地想到了KL-divergence。</p>
<p><img src="/pic/vae/3.png" /></p>
<p>该图是KL散度的推导过程。<span
class="math inline">\(\log(p_\theta(x))\)</span>是一个常数，因为无论x是什么分布，它对于<span
class="math inline">\(\mathcal{p}_\theta\)</span>都没有什么影响。因为KL散度是非负值，所以只要让<span
class="math inline">\(L(\theta,\phi;x)\)</span>值足够大，KL就会尽可能的小，两个度量的相似度也因此越高。L被称作variational
lower bound。</p>
<p><img src="/pic/vae/4.png" /></p>
<p>现在我们要做的就是尽可能地提升L，因此L可以作为模型训练时的loss
function。拆分后，左边一项就是一个期望值，右边一项正好是一个KL散度的表达式。左边期望E的含义是：给定一个latent
variable
z，输出x确实是我们输入的x的概率，也就是从z推导出的x和输入x要尽量接近。这一项被称为reconstruction
loss。第二项KL散度希望我们推得的representation和先验的latent
representation尽量接近。这一项被称为regularization loss。这个loss
function实际上可以写成一个期望的形式，而为了估计这种期望的导数，我们将使用蒙特卡罗方法。在下面我们将把中括号里的内容表示为<span
class="math inline">\(f(z)\)</span>，并假设z和<span
class="math inline">\(\phi\)</span>没有关系。</p>
<p><img src="/pic/vae/5.png" /></p>
<p>该图中，最上面一行公式在对<span
class="math inline">\(f(z)\)</span>的期望求导，用蒙特卡罗方法采样几个值求平均，最后可以当作期望值。下半张图是对等式推导的说明，注意下面的<span
class="math inline">\(\theta\)</span>意义等同于上式的<span
class="math inline">\(\phi\)</span>。根据作者的实验，如果使用这样的estimator，得到的结果具有高方差，更直观地说，会导致训练不稳定。</p>
<p><img src="/pic/vae/6.png" /></p>
<p>而在此基础上，作者提出了Generic Stochastic Gradient Variational Bayes
(SGVB) estimator，并且使用了Reparameterization Trick。在之前的Monte
Carlo estimator中，我们进行了一个非常强的假设，也就是f(z)和<span
class="math inline">\(\phi\)</span>无关，但实际上是有关的。在求导中的前一项我们无法处理。为了解决这个问题，作者引入了一个随机变量<span
class="math inline">\(\epsilon\)</span>，它和其它所有变量均无关系，用它来表示产生z(latent
parameter)时所有的随机性。在抽样产生z的过程中，所有的随机性都是由<span
class="math inline">\(\epsilon\)</span>产生的，而<span
class="math inline">\(\epsilon\)</span>和<span
class="math inline">\(\phi\)</span>没有任何关系。</p>
<p><img src="/pic/vae/7.png" /></p>
<p>从更加数学的角度，作者分析了Reparameterization
Trick。在左图中，根据<span
class="math inline">\(\phi\)</span>和x得到分布，然后从分布中抽样一个z。然而，怎么从采样中反向传递梯度，这显然无法做到。然而，如果把随机性移动到epsilon，先前的抽样过程采样就可以进行顺利的反向传播。</p>
<p><img src="/pic/vae/8.png" /></p>
<p>我们把z写成<span
class="math inline">\(\mathbf{z}^{(i,l)}=g_\phi(\mathbf{\epsilon}^{(i,l)},\mathbf{x}^{(i)})\)</span>的形式，然后按照<span
class="math inline">\(\epsilon^{(l)}\sim
p(\boldsymbol{\epsilon})\)</span>进行抽样。这样就得到了<span
class="math inline">\(\widetilde{\mathcal{L}}^A(\boldsymbol{\theta},\boldsymbol{\phi};\mathbf{x}^{(i)})\)</span>，也就是loss的形式，接下来只要两边求导即可。因为期望的distribution已经被替换成了<span
class="math inline">\(\epsilon\)</span>的distribution，所以跟<span
class="math inline">\(\phi\)</span>是没有关系的，可以直接求导。</p>
<p><img src="/pic/vae/9.png" /></p>
<p><img src="/pic/vae/10.png" /></p>
<p>对于loss，之前我们写成了KL divergence 和一个期望的和，而KL
divergence有很多优秀的性质。</p>
<p><img src="/pic/vae/11.png" /></p>
<p>随后把KL
divergence的两项合并起来，得到一个非常简单的loss。另一方面，我们是用最简单的MSE，使得给定z求出的x尽量逼近x，就可以最大化loss。这就是我们最终使用的SGVB</p>
<p><img src="/pic/vae/12.png" /></p>
<p>总框架：先输入一个x，经过由参数<span
class="math inline">\(\phi\)</span>决定的一个encoder，encoder会产生参数<span
class="math inline">\(\mu\)</span>和<span
class="math inline">\(\Sigma\)</span>，首先被用来计算KL
divergence，接着对它抽样产生一个z（此处使用epsilon产生一个随机的项）。得到z之后，作为decoder的输入，此处的decoder是由参数<span
class="math inline">\(\theta\)</span>来决定的。经过decoder之后，我们重建出了x，并算出输入输出x的MSE，作为LOSS的另一部分。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Timako</p>
  <div class="site-description" itemprop="description">Time heals</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">7</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Timako</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://muse.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a> 强力驱动
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
